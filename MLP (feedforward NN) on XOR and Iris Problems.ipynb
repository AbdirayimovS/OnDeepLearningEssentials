{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee3d42b",
   "metadata": {},
   "source": [
    "# XOR (\"exclusive OR\") Problem Solution\n",
    "\n",
    "f(0, 0) => 0 \n",
    "\n",
    "f(1, 1) => 0 \n",
    "\n",
    "f(0, 1) => 1\n",
    "\n",
    "f(1, 0) => 1 \n",
    "\n",
    "In fact, linear models can **not** learn it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "553f7f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 and mse=0.3025839505181337\n",
      "epoch=1000 and mse=0.01284320444022707\n",
      "epoch=2000 and mse=0.003287966924352181\n",
      "epoch=3000 and mse=0.0017330617326462823\n",
      "epoch=4000 and mse=0.001147102822539652\n",
      "epoch=5000 and mse=0.0008463523080943702\n",
      "epoch=6000 and mse=0.000666258630898571\n",
      "epoch=7000 and mse=0.0005469114799749259\n",
      "epoch=8000 and mse=0.0004624541798224118\n",
      "epoch=9000 and mse=0.00039970804778994396\n",
      "[[0.02970087]\n",
      " [0.98607326]\n",
      " [0.98606514]\n",
      " [0.01165664]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "np.random.seed(42)\n",
    "learning_rate = 0.1 \n",
    "epochs = 10_000 \n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "def sigmoid_derivative(z):\n",
    "    return z * (1 - z)\n",
    "\n",
    "\n",
    "X = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "\n",
    "\n",
    "input_layer_neurons = 2 \n",
    "hidden_layer_neurons = 4\n",
    "output_layer_neurons = 1\n",
    "\n",
    "hidden_weights = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n",
    "hidden_bias = np.zeros((1, hidden_layer_neurons))\n",
    "output_weights = np.random.uniform(size=(hidden_layer_neurons, output_layer_neurons)) \n",
    "output_bias = np.zeros((1, output_layer_neurons))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # forward propagation \n",
    "    \n",
    "    hidden_layer_input = np.dot(X, hidden_weights) + hidden_bias\n",
    "    hidden_layer_activation = relu(hidden_layer_input)\n",
    "    output_layer_input = np.dot(hidden_layer_activation, output_weights) + output_bias\n",
    "    predicted_outputs = sigmoid(output_layer_input)\n",
    "    \n",
    "    # loss calculation \n",
    "    error = y - predicted_outputs\n",
    "    mse = np.mean(np.square(error))\n",
    "    \n",
    "    # backpropagation \n",
    "    \n",
    "    d_predicted_outputs = error * sigmoid_derivative(predicted_outputs)\n",
    "    \n",
    "    error_hidden_layer = d_predicted_outputs.dot(output_weights.T)\n",
    "    d_hidden_layer = error_hidden_layer * relu_derivative(hidden_layer_activation)\n",
    "    \n",
    "    output_weights = output_weights + learning_rate * hidden_layer_activation.T.dot(d_predicted_outputs)\n",
    "    output_bias = output_bias + learning_rate * np.sum(d_predicted_outputs, axis=0, keepdims=True)\n",
    "    \n",
    "    hidden_weights = hidden_weights + learning_rate * X.T.dot(d_hidden_layer)\n",
    "    hidden_bias = hidden_bias + learning_rate * np.sum(d_hidden_layer, axis=0, keepdims=True)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"{epoch=} and {mse=}\")\n",
    "\n",
    "print(predicted_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "822fad72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98607516]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_input = np.dot(np.array([[1, 0]]), hidden_weights) + hidden_bias\n",
    "hidden_layer_activation = relu(hidden_layer_input)\n",
    "output_layer_input = np.dot(hidden_layer_activation, output_weights) + output_bias\n",
    "sigmoid(output_layer_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d952a3c4",
   "metadata": {},
   "source": [
    "# Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2255884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c4899d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data \n",
    "y = iris.target\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35911d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "84dd2d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 and loss=5.635810831339497\n",
      "epoch=1000 and loss=0.07471408351855109\n",
      "epoch=2000 and loss=0.06213335675673911\n",
      "epoch=3000 and loss=0.06043267321214014\n",
      "epoch=4000 and loss=0.05939030463394691\n",
      "epoch=5000 and loss=0.0585932656597506\n",
      "epoch=6000 and loss=0.05783386799148585\n",
      "epoch=7000 and loss=0.05721356849612756\n",
      "epoch=8000 and loss=0.05660345108130884\n",
      "epoch=9000 and loss=0.055970161263423385\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "learning_rate = 0.1 \n",
    "epochs = 10_000 \n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z, 0)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def softmax(z):\n",
    "    exps = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exps/np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def softmax_derivative(z):\n",
    "    pass\n",
    "\n",
    "input_layer_neurons = X_train.shape[1]\n",
    "hidden_layer_neurons = 10 \n",
    "output_neurons = y_train.shape[1]\n",
    "\n",
    "hidden_weights = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n",
    "hidden_bias = np.random.uniform(size=(1, hidden_layer_neurons))\n",
    "\n",
    "output_weights = np.random.uniform(size=(hidden_layer_neurons, output_neurons))\n",
    "output_bias = np.random.uniform(size=(1, output_neurons))\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # feedforward propagation \n",
    "    hidden_layer_input = np.dot(X_train, hidden_weights) + hidden_bias\n",
    "    hidden_layer_activation = relu(hidden_layer_input)\n",
    "    output_layer_input = np.dot(hidden_layer_activation, output_weights) + output_bias\n",
    "    predicted_outputs = softmax(output_layer_input)\n",
    "    \n",
    "    loss = - np.mean(np.sum(y_train * np.log(predicted_outputs + 1e-8), axis=1))\n",
    "    error = y_train  - predicted_outputs # minus_error = y_train - predicted_outputs \n",
    "    \n",
    "    d_output_weights = hidden_layer_activation.T.dot(error) / X_train.shape[0]\n",
    "    d_output_bias = np.sum(error, axis=0, keepdims=True) / X_train.shape[0]\n",
    "    \n",
    "    error_hidden_layer = error.dot(output_weights.T) * relu_derivative(hidden_layer_activation)\n",
    "    d_hidden_weights = X_train.T.dot(error_hidden_layer) / X_train.shape[0]\n",
    "    d_hidden_bias = np.sum(error_hidden_layer, axis=0, keepdims=True) / X_train.shape[0]\n",
    "    \n",
    "    output_weights += learning_rate * d_output_weights \n",
    "    output_bias += learning_rate * d_output_bias \n",
    "    hidden_weights += learning_rate * d_hidden_weights\n",
    "    hidden_bias += learning_rate * d_hidden_bias\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"{epoch=} and {loss=}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "96c494f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_input = np.dot(X_test, hidden_weights) + hidden_bias\n",
    "hidden_layer_activation = relu(hidden_layer_input)\n",
    "output_layer_input = np.dot(hidden_layer_activation, output_weights) + output_bias\n",
    "predicted_test_outputs = softmax(output_layer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a55f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9d5f2e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_hat = np.argmax(predicted_test_outputs, axis=1)\n",
    "y_test_argmax = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy_score(y_test_argmax, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b774c722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
